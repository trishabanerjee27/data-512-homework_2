{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name                                                url  \\\n",
      "0        Majah Ha Adrif       https://en.wikipedia.org/wiki/Majah_Ha_Adrif   \n",
      "1     Haroon al-Afghani    https://en.wikipedia.org/wiki/Haroon_al-Afghani   \n",
      "2           Tayyab Agha          https://en.wikipedia.org/wiki/Tayyab_Agha   \n",
      "3  Khadija Zahra Ahmadi  https://en.wikipedia.org/wiki/Khadija_Zahra_Ah...   \n",
      "4        Aziza Ahmadyar       https://en.wikipedia.org/wiki/Aziza_Ahmadyar   \n",
      "\n",
      "       country  \n",
      "0  Afghanistan  \n",
      "1  Afghanistan  \n",
      "2  Afghanistan  \n",
      "3  Afghanistan  \n",
      "4  Afghanistan  \n",
      "         Geography  Population\n",
      "0            WORLD      8009.0\n",
      "1           AFRICA      1453.0\n",
      "2  NORTHERN AFRICA       256.0\n",
      "3          Algeria        46.8\n",
      "4            Egypt       105.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Geography     233\n",
       "Population    233\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load the politicians data\n",
    "politicians_df = pd.read_csv('../data/politicians_by_country_AUG.2024.csv')\n",
    "\n",
    "#Load the population data\n",
    "population_df = pd.read_csv('../data/population_by_country_AUG.2024.csv')\n",
    "\n",
    "#View the first few rows of each dataset\n",
    "print(politicians_df.head())\n",
    "politicians_df.count()\n",
    "\n",
    "print(population_df.head())\n",
    "population_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "44\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "#Check for duplicates based on all columns\n",
    "duplicate_politicians_all = politicians_df[politicians_df.duplicated()]\n",
    "print(len(duplicate_politicians_all))\n",
    "\n",
    "#Check for duplicates based on name + url\n",
    "duplicate_politicians_name = politicians_df[politicians_df.duplicated(subset=['name'])]\n",
    "duplicate_politicians_url = politicians_df[politicians_df.duplicated(subset=['url'])]\n",
    "print(len(duplicate_politicians_name))\n",
    "print(len(duplicate_politicians_url))\n",
    "\n",
    "#This means that the only thing setting them apart is different countries for the same name and url.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Check for duplicates based on all columns\n",
    "duplicate_population_all = population_df[population_df.duplicated()]\n",
    "print(len(duplicate_population_all))\n",
    "\n",
    "#Check for duplicates based on geography\n",
    "duplicate_population_geo = population_df[population_df.duplicated(subset=['Geography'])]\n",
    "print(len(duplicate_population_geo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44 of politician appear in mulitple countries (2 or more). This has been decided (based on the Wikipedia API)\n",
    "#either by their nationalities or the next country served, so it makes sense to have them be a part of both/all \n",
    "#the countries their names appear in\n",
    "\n",
    "# however we keep a copy of the duplicate politicians\n",
    "combined_duplicates = pd.concat([duplicate_politicians_name, duplicate_politicians_url]).drop_duplicates()\n",
    "combined_duplicates.to_csv('combined_duplicates_politicians.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Geography     209\n",
       "Population    209\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create a new column to check if Geography is in all caps\n",
    "population_df['is_region'] = population_df['Geography'].apply(lambda x: x.isupper())\n",
    "\n",
    "df_region = population_df[population_df['is_region'] == True].copy()\n",
    "df_country = population_df[population_df['is_region'] == False].copy()\n",
    "\n",
    "# Drop the helper column\n",
    "df_region = df_region.drop(columns=['is_region'])\n",
    "df_country = df_country.drop(columns=['is_region'])\n",
    "\n",
    "df_country.count()\n",
    "# df_region.to_csv('population_by_region.csv', index=False)\n",
    "# df_country.to_csv('population_by_country.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name       0\n",
      "url        0\n",
      "country    0\n",
      "dtype: int64\n",
      "Geography     0\n",
      "Population    0\n",
      "is_region     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values_politicians = politicians_df.isnull().sum()\n",
    "print(missing_values_politicians)\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values_population = population_df.isnull().sum()\n",
    "print(missing_values_population)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<tbaner@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "# PageInfo Request Template\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",  # Placeholder for article title\n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": \"url|talkid\"\n",
    "}\n",
    "\n",
    "# Function to request page info and retrieve revision ID\n",
    "def request_pageinfo_per_article(article_title):\n",
    "    request_template = PAGEINFO_PARAMS_TEMPLATE.copy()\n",
    "    request_template['titles'] = article_title\n",
    "\n",
    "    if API_THROTTLE_WAIT > 0.0:\n",
    "        time.sleep(API_THROTTLE_WAIT)  # Respect API rate limits\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_ENWIKIPEDIA_ENDPOINT, headers=REQUEST_HEADERS, params=request_template)\n",
    "        json_response = response.json()\n",
    "        \n",
    "        # Extract the page info\n",
    "        pages = json_response[\"query\"][\"pages\"]\n",
    "        for page_id, page_info in pages.items():\n",
    "            revision_id = page_info.get(\"lastrevid\", None)  # Retrieve the revision ID\n",
    "            if revision_id:\n",
    "                return revision_id\n",
    "            else:\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page info for {article_title}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORES API Constants\n",
    "ORES_ENDPOINT = \"https://ores.wikimedia.org/v3/scores/enwiki/\"\n",
    "ORES_MODEL = \"wp10\"  # The model used for quality predictions\n",
    "\n",
    "# Function to request article quality using ORES API\n",
    "def get_ores_quality_prediction(article_title, revision_id):\n",
    "    try:\n",
    "        ores_url = f\"{ORES_ENDPOINT}?models={ORES_MODEL}&revids={revision_id}\"\n",
    "        response = requests.get(ores_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            scores = data['enwiki']['scores'][str(revision_id)]['wp10']['score']['prediction']\n",
    "            return scores\n",
    "        else:\n",
    "            print(f\"Failed to get ORES score for {article_title} (Revision ID: {revision_id})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting ORES score for {article_title}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.11%\n"
     ]
    }
   ],
   "source": [
    "#list to log articles without scores\n",
    "error_log = []\n",
    "\n",
    "#dd columns to store revision ID and quality score\n",
    "politicians_df['revision_id'] = None\n",
    "politicians_df['quality_score'] = None\n",
    "\n",
    "#loop through each politician and get revision ID and quality score\n",
    "for index, row in politicians_df.iterrows():\n",
    "    article_title = row['url'].split('/')[-1]\n",
    "    revision_id = request_pageinfo_per_article(article_title)\n",
    "    \n",
    "    if revision_id:\n",
    "        quality_score = get_ores_quality_prediction(article_title, revision_id)\n",
    "        politicians_df.at[index, 'revision_id'] = revision_id\n",
    "        politicians_df.at[index, 'quality_score'] = quality_score\n",
    "    else:\n",
    "        error_log.append(article_title)\n",
    "\n",
    "#save the error log for any missing articles\n",
    "with open('../data/ores_error_log.txt', 'w') as log_file:\n",
    "    log_file.write(\"\\n\".join(error_log))\n",
    "\n",
    "politicians_df.to_csv('../data/politicians_with_quality.csv', index=False)\n",
    "\n",
    "#calculate error rate\n",
    "error_rate = len(error_log) / len(politicians_df)\n",
    "print(f\"Error rate: {error_rate:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
