{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "We begin by loading two datasets:\n",
    "\n",
    "1. **Politicians Data**: This dataset (`politicians_by_country_AUG.2024.csv`) contains information about Wikipedia articles on politicians from various countries, including article titles, countries, revision IDs, and predicted quality scores provided by the ORES API.\n",
    "\n",
    "2. **Population Data**: This dataset (`population_by_country_AUG.2024.csv`) provides population information for countries and regions, with population values reported in millions.\n",
    "\n",
    "The following code loads both datasets and prints the first few rows of each to inspect their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name                                                url  \\\n",
      "0        Majah Ha Adrif       https://en.wikipedia.org/wiki/Majah_Ha_Adrif   \n",
      "1     Haroon al-Afghani    https://en.wikipedia.org/wiki/Haroon_al-Afghani   \n",
      "2           Tayyab Agha          https://en.wikipedia.org/wiki/Tayyab_Agha   \n",
      "3  Khadija Zahra Ahmadi  https://en.wikipedia.org/wiki/Khadija_Zahra_Ah...   \n",
      "4        Aziza Ahmadyar       https://en.wikipedia.org/wiki/Aziza_Ahmadyar   \n",
      "\n",
      "       country  \n",
      "0  Afghanistan  \n",
      "1  Afghanistan  \n",
      "2  Afghanistan  \n",
      "3  Afghanistan  \n",
      "4  Afghanistan  \n",
      "         Geography  Population\n",
      "0            WORLD      8009.0\n",
      "1           AFRICA      1453.0\n",
      "2  NORTHERN AFRICA       256.0\n",
      "3          Algeria        46.8\n",
      "4            Egypt       105.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Geography     233\n",
       "Population    233\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load the politicians data\n",
    "politicians_df = pd.read_csv('../data/politicians_by_country_AUG.2024.csv')\n",
    "\n",
    "#Load the population data\n",
    "population_df = pd.read_csv('../data/population_by_country_AUG.2024.csv')\n",
    "\n",
    "#View the first few rows of each dataset\n",
    "print(politicians_df.head())\n",
    "politicians_df.count()\n",
    "\n",
    "print(population_df.head())\n",
    "population_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Duplicates in the Politicians Data\n",
    "\n",
    "To ensure data integrity, we check for duplicate entries in the politicians dataset. Duplicate records can arise from various sources, such as errors in data scraping or discrepancies in data entry. Identifying these duplicates is crucial for accurate analysis.\n",
    "\n",
    "The following steps are taken to identify potential duplicates:\n",
    "\n",
    "1. **Check for Duplicates Based on All Columns**:  \n",
    "   We first check for any duplicate entries where all columns (e.g., name, URL, country, revision ID) are identical.\n",
    "\n",
    "2. **Check for Duplicates Based on Name and URL**:  \n",
    "   In some cases, articles with the same name and URL may exist for different countries (e.g., the same politician could be listed in multiple countries due to dual citizenship or other reasons). We specifically check for duplicates based on the `name` and `url` columns to identify such cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "44\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "#Check for duplicates based on all columns\n",
    "duplicate_politicians_all = politicians_df[politicians_df.duplicated()]\n",
    "print(len(duplicate_politicians_all))\n",
    "\n",
    "#Check for duplicates based on name + url\n",
    "duplicate_politicians_name = politicians_df[politicians_df.duplicated(subset=['name'])]\n",
    "duplicate_politicians_url = politicians_df[politicians_df.duplicated(subset=['url'])]\n",
    "print(len(duplicate_politicians_name))\n",
    "print(len(duplicate_politicians_url))\n",
    "\n",
    "#This means that the only thing setting them apart is different countries for the same name and url.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Duplicates in the Population Data\n",
    "\n",
    "To ensure the population dataset is clean and free from duplicate entries, we perform the following checks:\n",
    "\n",
    "1. **Check for Duplicates Based on All Columns**:  \n",
    "   We first check for any duplicate entries where all columns (e.g., Geography and Population) are identical. This ensures that there are no exact duplicates in the dataset.\n",
    "\n",
    "2. **Check for Duplicates Based on Geography**:  \n",
    "   We also check for duplicates based solely on the `Geography` column, which represents the country or region name. This will help identify cases where the same country or region might have multiple entries, possibly with different population values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Check for duplicates based on all columns\n",
    "duplicate_population_all = population_df[population_df.duplicated()]\n",
    "print(len(duplicate_population_all))\n",
    "\n",
    "#Check for duplicates based on geography\n",
    "duplicate_population_geo = population_df[population_df.duplicated(subset=['Geography'])]\n",
    "print(len(duplicate_population_geo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Politicians Appearing in Multiple Countries\n",
    "\n",
    "During the data cleaning process, we identified 44 politicians who appear in multiple countries. This duplication is likely due to their serving roles in different countries, either through holding multiple nationalities or by having political positions in various countries. As such, it is logical to include them in all countries they are associated with.\n",
    "\n",
    "#### Steps Taken:\n",
    "1. **Retaining Duplicates**:  \n",
    "   Rather than removing these entries, we keep a copy of the duplicate politicians. This ensures that they are accurately represented in all relevant countries for our analysis. The decision is based on information retrieved via the Wikipedia API, which shows that these politicians serve or are associated with multiple countries.\n",
    "\n",
    "2. **Saving the Duplicate Politicians**:  \n",
    "   We create a combined dataset of these duplicate entries and save it for future reference. The duplicate politicians, based on either name or URL, are concatenated and written to a separate CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44 of politician appear in mulitple countries (2 or more). This has been decided (based on the Wikipedia API)\n",
    "#either by their nationalities or the next country served, so it makes sense to have them be a part of both/all \n",
    "#the countries their names appear in\n",
    "\n",
    "# however we keep a copy of the duplicate politicians\n",
    "combined_duplicates = pd.concat([duplicate_politicians_name, duplicate_politicians_url]).drop_duplicates()\n",
    "combined_duplicates.to_csv('../data/combined_duplicates_politicians.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Population Data with Continents and Regions\n",
    "\n",
    "In this section, we expand the population dataset by adding `Continent` and `Region` columns. The population data contains a mix of continents, regions, and countries, so the goal is to assign each country to its respective continent and region. Some special cases (like \"Northern America\" and \"Oceania\") are handled explicitly to ensure proper assignment.\n",
    "\n",
    "#### Steps:\n",
    "1. **Copying the Data**:  \n",
    "   We begin by creating a copy of the original population DataFrame to avoid modifying the original data.\n",
    "\n",
    "2. **Defining Special Cases**:  \n",
    "   Some entries, such as \"Northern America\" and \"Oceania\", are regions and continents simultaneously. These are handled through the `special_cases` dictionary.\n",
    "\n",
    "3. **Assigning Continents and Regions**:  \n",
    "   We loop through the rows of the DataFrame, checking whether a row represents a continent, region, or country:\n",
    "   - If the `Geography` is in uppercase, it indicates either a continent or region.\n",
    "   - Countries are assigned the current `Continent` and `Region` based on the most recent continent and region encountered in the loop.\n",
    "\n",
    "4. **Filtering and Cleaning**:  \n",
    "   Once the `Continent` and `Region` columns are populated, we filter out rows that represent continents or regions themselves, keeping only the country-level data. Finally, the `is_region` helper column is dropped.\n",
    "\n",
    "5. **Saving the Expanded Data**:  \n",
    "   The expanded DataFrame, now containing `Country`, `Continent`, and `Region` information, is saved as `population_expanded.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country  Population Continent           Region\n",
      "0  Algeria        46.8    AFRICA  NORTHERN AFRICA\n",
      "1    Egypt       105.2    AFRICA  NORTHERN AFRICA\n",
      "2    Libya         6.9    AFRICA  NORTHERN AFRICA\n",
      "3  Morocco        37.0    AFRICA  NORTHERN AFRICA\n",
      "4    Sudan        48.1    AFRICA  NORTHERN AFRICA\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "population_df_copy = population_df.copy()\n",
    "population_df_copy['Continent'] = None\n",
    "population_df_copy['Region'] = None\n",
    "\n",
    "special_cases = {\n",
    "    'NORTHERN AMERICA': 'NORTHERN AMERICA',\n",
    "    'OCEANIA': 'OCEANIA'\n",
    "}\n",
    "\n",
    "#variables to store the current continent and region\n",
    "current_continent = None\n",
    "current_region = None\n",
    "\n",
    "#iterate over the rows of the df and assign the continent and region\n",
    "for index, row in population_df_copy.iterrows():\n",
    "    geography = row['Geography']\n",
    "    \n",
    "    if geography.isupper():\n",
    "        if geography == 'WORLD':\n",
    "            continue #skipping world\n",
    "        elif geography in ['AFRICA', 'NORTHERN AMERICA','LATIN AMERICA AND THE CARIBBEAN', 'EUROPE', 'ASIA', 'OCEANIA']:\n",
    "\n",
    "            current_continent = geography\n",
    "            current_region = special_cases.get(geography, None)\n",
    "   \n",
    "        else:\n",
    "            current_region = geography\n",
    "    else:\n",
    "        population_df_copy.at[index, 'Continent'] = current_continent\n",
    "        population_df_copy.at[index, 'Region'] = current_region\n",
    "\n",
    "population_df_copy.rename(columns={'Geography': 'Country'}, inplace=True)\n",
    "\n",
    "#filter out rows that represent continents or regions\n",
    "#we already have the regions and continents information populated, so we will drop rows where continent or region is None.\n",
    "population_df_new = population_df_copy.dropna(subset=['Continent', 'Region'])\n",
    "\n",
    "population_df_new.reset_index(drop=True, inplace=True)\n",
    "\n",
    "if 'is_region' in population_df_new.columns:\n",
    "    population_df_new = population_df_new.drop(columns=['is_region'])\n",
    "population_df_new.to_csv('../data/population_expanded.csv', index=False)\n",
    "\n",
    "print(population_df_new.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Values in the Datasets\n",
    "\n",
    "Before proceeding with any data analysis, it is important to identify any missing values in the datasets. Missing values can skew the results, so identifying them early on helps us decide how to handle such cases in further processing.\n",
    "\n",
    "#### Steps:\n",
    "1. **Checking Missing Values in the Politicians Dataset**:  \n",
    "   We use the `isnull()` function combined with `sum()` to calculate the number of missing values in each column of the `politicians_df`.\n",
    "\n",
    "2. **Checking Missing Values in the Population Dataset**:  \n",
    "   Similarly, we apply the same method to the `population_df` to identify missing values in the population data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name             0\n",
      "url              0\n",
      "country          0\n",
      "revision_id      8\n",
      "quality_score    8\n",
      "dtype: int64\n",
      "Geography     0\n",
      "Population    0\n",
      "is_region     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for missing values in each column\n",
    "missing_values_politicians = politicians_df.isnull().sum()\n",
    "print(missing_values_politicians)\n",
    "\n",
    "#check for missing values in each column\n",
    "missing_values_population = population_df.isnull().sum()\n",
    "print(missing_values_population)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Article Page Info from Wikipedia API\n",
    "\n",
    "To enrich our analysis with article quality predictions, we first need to obtain the most recent revision ID of each article in the Wikipedia dataset. We do this by making requests to the MediaWiki API using the article titles provided in the dataset.\n",
    "\n",
    "#### Steps:\n",
    "1. **Setting Up API Requests**:  \n",
    "   We use the MediaWiki API to fetch page information for each article. The API endpoint for this is `https://en.wikipedia.org/w/api.php`, and the parameters specify that we want the page URL and revision ID.\n",
    "\n",
    "2. **API Throttling**:  \n",
    "   To avoid overwhelming the API, we implement a throttle with a calculated wait time between requests, ensuring compliance with Wikipedia’s request rate limits. \n",
    "\n",
    "3. **User-Agent**:  \n",
    "   Every API request includes a custom `User-Agent` header that identifies our project and affiliation (University of Washington, MSDS program).\n",
    "\n",
    "4. **Functionality**:  \n",
    "   The function `request_pageinfo_per_article` takes an article title as input and retrieves its most recent revision ID by sending a request to the Wikipedia API. If the article title is valid, it extracts and returns the revision ID, or `None` if the revision ID cannot be found.\n",
    "\n",
    "5. **Error Handling**:  \n",
    "   If there’s an issue with the request (e.g., connection failure or invalid response), the function catches the exception, logs an error message, and returns `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "#constants\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "API_LATENCY_ASSUMED = 0.002\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<tbaner@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "#pageInfo Request Template\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",  \n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": \"url|talkid\"\n",
    "}\n",
    "\n",
    "#function to request page info and retrieve revision ID\n",
    "def request_pageinfo_per_article(article_title):\n",
    "    request_template = PAGEINFO_PARAMS_TEMPLATE.copy()\n",
    "    request_template['titles'] = article_title\n",
    "\n",
    "    if API_THROTTLE_WAIT > 0.0:\n",
    "        time.sleep(API_THROTTLE_WAIT)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_ENWIKIPEDIA_ENDPOINT, headers=REQUEST_HEADERS, params=request_template)\n",
    "        json_response = response.json()\n",
    "        \n",
    "        #extract the page info\n",
    "        pages = json_response[\"query\"][\"pages\"]\n",
    "        for page_id, page_info in pages.items():\n",
    "            revision_id = page_info.get(\"lastrevid\", None)\n",
    "            if revision_id:\n",
    "                return revision_id\n",
    "            else:\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page info for {article_title}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Article Quality Predictions from ORES API\n",
    "\n",
    "After obtaining the latest revision ID for each Wikipedia article, we request the ORES API to predict the article's quality. ORES (Objective Revision Evaluation Service) uses machine learning models to provide quality predictions based on Wikipedia’s assessment scale (e.g., \"FA\", \"GA\", \"B\", \"C\", \"Start\", \"Stub\").\n",
    "\n",
    "#### Steps:\n",
    "1. **Setting Up API Requests**:  \n",
    "   The ORES API is used to fetch the article quality score. The endpoint is `https://ores.wikimedia.org/v3/scores/enwiki/`, with the quality prediction model being `wp10`. The request includes the revision ID of the article as a query parameter to get the prediction.\n",
    "\n",
    "2. **Functionality**:  \n",
    "   The function `get_ores_quality_prediction` takes an article title and its revision ID as inputs, constructs the API request URL, and sends the request to the ORES API. If the request is successful, it extracts the quality score from the JSON response. The quality score is one of Wikipedia's quality classes such as \"FA\" (featured article), \"GA\" (good article), etc.\n",
    "\n",
    "3. **Error Handling**:  \n",
    "   If there’s an issue retrieving the score (e.g., connection failure or an invalid response), the function catches the exception, logs an error message, and returns `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORES API Constants\n",
    "ORES_ENDPOINT = \"https://ores.wikimedia.org/v3/scores/enwiki/\"\n",
    "ORES_MODEL = \"wp10\"  # The model used for quality predictions\n",
    "\n",
    "#function to request article quality using ORES API\n",
    "def get_ores_quality_prediction(article_title, revision_id):\n",
    "    try:\n",
    "        ores_url = f\"{ORES_ENDPOINT}?models={ORES_MODEL}&revids={revision_id}\"\n",
    "        response = requests.get(ores_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            scores = data['enwiki']['scores'][str(revision_id)]['wp10']['score']['prediction']\n",
    "            return scores\n",
    "        else:\n",
    "            print(f\"Failed to get ORES score for {article_title} (Revision ID: {revision_id})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting ORES score for {article_title}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Revision IDs and Quality Scores for Politicians' Articles\n",
    "\n",
    "To enrich the dataset with article revision IDs and ORES-predicted quality scores, this section of the code iterates over each row in the `politicians_df` DataFrame, retrieving the revision ID and article quality score for each Wikipedia article. Any articles that fail to retrieve a revision ID or quality score are logged for further review.\n",
    "\n",
    "#### Steps:\n",
    "1. **Setting Up Error Log**:  \n",
    "   We initialize an empty list `error_log` to track articles for which revision IDs or quality scores could not be retrieved.\n",
    "\n",
    "2. **Adding New Columns**:  \n",
    "   Two new columns, `revision_id` and `quality_score`, are added to the `politicians_df` DataFrame to store the retrieved revision ID and predicted article quality, respectively.\n",
    "\n",
    "3. **Loop Through Each Article**:  \n",
    "   The code loops through each row in the DataFrame, extracting the article title from the URL and calling two functions: \n",
    "   - `request_pageinfo_per_article` to get the latest revision ID.\n",
    "   - `get_ores_quality_prediction` to obtain the article quality score from the ORES API using the revision ID.\n",
    "\n",
    "4. **Handling Missing Data**:  \n",
    "   If a revision ID cannot be retrieved, the article title is added to the `error_log`. After completing the loop, the `error_log` is saved to a file (`ores_error_log.txt`) for review.\n",
    "\n",
    "5. **Saving the Updated Data**:  \n",
    "   Once the DataFrame is updated with the retrieved revision IDs and quality scores, the updated dataset is saved to `politicians_with_quality.csv`.\n",
    "\n",
    "6. **Calculating Error Rate**:  \n",
    "   The code calculates the error rate as the proportion of articles for which revision IDs or quality scores could not be retrieved, and prints it for review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.11%\n"
     ]
    }
   ],
   "source": [
    "#list to log articles without scores\n",
    "error_log = []\n",
    "\n",
    "#dd columns to store revision ID and quality score\n",
    "politicians_df['revision_id'] = None\n",
    "politicians_df['quality_score'] = None\n",
    "\n",
    "#loop through each politician and get revision ID and quality score\n",
    "for index, row in politicians_df.iterrows():\n",
    "    article_title = row['url'].split('/')[-1]\n",
    "    revision_id = request_pageinfo_per_article(article_title)\n",
    "    \n",
    "    if revision_id:\n",
    "        quality_score = get_ores_quality_prediction(article_title, revision_id)\n",
    "        politicians_df.at[index, 'revision_id'] = revision_id\n",
    "        politicians_df.at[index, 'quality_score'] = quality_score\n",
    "    else:\n",
    "        error_log.append(article_title)\n",
    "\n",
    "#save the error log for any missing articles\n",
    "with open('../data/ores_error_log.txt', 'w') as log_file:\n",
    "    log_file.write(\"\\n\".join(error_log))\n",
    "\n",
    "politicians_df.to_csv('../data/politicians_with_quality.csv', index=False)\n",
    "\n",
    "#calculate error rate\n",
    "error_rate = len(error_log) / len(politicians_df)\n",
    "print(f\"Error rate: {error_rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Entries for China and Guinea-Bissau, While Excluding Korea\n",
    "\n",
    "In this section of the code, we handled special cases by combining entries for countries that had different variations in their names between the Wikipedia politicians dataset and the population dataset. For example, we merged entries for China and Guinea-Bissau, but chose to leave Korea out of the merge due to the ambiguity between North and South Korea in the articles.\n",
    "\n",
    "#### Key Steps:\n",
    "1. **Handling Special Cases with `name_map`:**  \n",
    "   We created a dictionary `name_map` that standardizes the country names. Variants like \"China (Hong Kong SAR)\" and \"China (Macao SAR)\" are both mapped to \"China,\" and multiple spellings of Guinea-Bissau are normalized. We excluded Korea because merging \"North Korea\" and \"South Korea\" into a single \"Korea\" would introduce bias due to political differences.\n",
    "\n",
    "2. **Applying the Name Mapping:**  \n",
    "   We applied the `name_map` to both the `politicians_df` and `population_df_new` DataFrames to standardize the country names before performing any merges.\n",
    "\n",
    "3. **Identifying Unmatched Countries:**  \n",
    "   After merging the two datasets based on country names, we identified unmatched countries from both datasets and saved them into the file `wp_countries-no_match.txt`. This file lists all countries that were not found in either dataset.\n",
    "\n",
    "4. **Inner Merge of Full Data:**  \n",
    "   We performed an inner merge of the two DataFrames, `politicians_df` and `population_df_new`, based on the standardized country names. This merge results in a dataset that includes only countries that appear in both datasets.\n",
    "\n",
    "5. **Cleaning the Merged Data:**  \n",
    "   The resulting merged DataFrame was cleaned to include only the relevant columns: `country`, `region`, `population`, `article_title`, `revision_id`, and `article_quality`. The final cleaned dataset is saved to `wp_politicians_by_country.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/g7llzl3517q8v_lqg7_6_5_40000gn/T/ipykernel_29650/1044880352.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_df_clean.rename(columns={\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>population</th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Majah Ha Adrif</td>\n",
       "      <td>1233202991</td>\n",
       "      <td>Start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Haroon al-Afghani</td>\n",
       "      <td>1230459615</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Tayyab Agha</td>\n",
       "      <td>1225661708</td>\n",
       "      <td>Start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Khadija Zahra Ahmadi</td>\n",
       "      <td>1234741562</td>\n",
       "      <td>Stub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Aziza Ahmadyar</td>\n",
       "      <td>1195651393</td>\n",
       "      <td>Start</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country      region  population         article_title revision_id  \\\n",
       "0  Afghanistan  SOUTH ASIA        42.4        Majah Ha Adrif  1233202991   \n",
       "1  Afghanistan  SOUTH ASIA        42.4     Haroon al-Afghani  1230459615   \n",
       "2  Afghanistan  SOUTH ASIA        42.4           Tayyab Agha  1225661708   \n",
       "3  Afghanistan  SOUTH ASIA        42.4  Khadija Zahra Ahmadi  1234741562   \n",
       "4  Afghanistan  SOUTH ASIA        42.4        Aziza Ahmadyar  1195651393   \n",
       "\n",
       "  article_quality  \n",
       "0           Start  \n",
       "1               B  \n",
       "2           Start  \n",
       "3            Stub  \n",
       "4           Start  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining the China entries and Guinea -Bissau ones. Leaving Korea out (as the articles do not mention \n",
    "#whether the politicians are in North or South Korea so clubbing them into Korea will add bias)\n",
    "\n",
    "#adding a mapping\n",
    "name_map = {\n",
    "    \"guineabissau\": \"Guinea-Bissau\",\n",
    "    \"GuineaBissau\": \"Guinea-Bissau\",\n",
    "    \"China (Hong Kong SAR)\": \"China\",\n",
    "    \"China (Macao SAR)\": \"China\"\n",
    "}\n",
    "\n",
    "#apply the name_map to both dfs before merging\n",
    "politicians_df['country'] = politicians_df['country'].replace(name_map)\n",
    "population_df_new['Country'] = population_df_new['Country'].replace(name_map)\n",
    "\n",
    "#extract unique countries from both dfs\n",
    "unique_countries_politicians = politicians_df['country'].drop_duplicates()\n",
    "unique_countries_population = population_df_new['Country'].drop_duplicates()\n",
    "\n",
    "#perform an inner merge based on unique country names\n",
    "merged_df = pd.merge(\n",
    "    unique_countries_politicians.to_frame(name='country'),\n",
    "    unique_countries_population.to_frame(name='Country'),\n",
    "    how='inner',\n",
    "    left_on='country',\n",
    "    right_on='Country'\n",
    ")\n",
    "\n",
    "#identify unmatched countries by comparing the two unique sets\n",
    "unmatched_countries_left = unique_countries_politicians[~unique_countries_politicians.isin(merged_df['country'])]\n",
    "unmatched_countries_right = unique_countries_population[~unique_countries_population.isin(merged_df['Country'])]\n",
    "\n",
    "# ombine all unmatched countries into a list\n",
    "unmatched_countries = list(unmatched_countries_left) + list(unmatched_countries_right)\n",
    "\n",
    "#saving unmatched countries\n",
    "with open('../data/wp_countries-no_match.txt', 'w') as f:\n",
    "    for country in unmatched_countries:\n",
    "        if pd.notna(country):  # Only write valid country names\n",
    "            f.write(f\"{country}\\n\")\n",
    "\n",
    "#perform full inner join between the original dfs based on the matched countries\n",
    "merged_full_df = pd.merge(\n",
    "    politicians_df,\n",
    "    population_df_new,\n",
    "    how='inner',\n",
    "    left_on='country',\n",
    "    right_on='Country'\n",
    ")\n",
    "\n",
    "#cleaning the merged data to include only required columns\n",
    "merged_df_clean = merged_full_df[['country', 'Region', 'Population', 'name', 'revision_id', 'quality_score']]\n",
    "merged_df_clean.rename(columns={\n",
    "    'Region': 'region',\n",
    "    'Population': 'population',\n",
    "    'name': 'article_title',\n",
    "    'quality_score': 'article_quality'\n",
    "}, inplace=True)\n",
    "\n",
    "merged_df_clean.to_csv('../data/wp_politicians_by_country.csv', index=False)\n",
    "merged_df_clean.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Rows with Missing `revision_id`\n",
    "\n",
    "In this step, we handle rows that are missing the `revision_id`, which are also logged in the `ores_error_log.txt`. Since a `revision_id` is required to retrieve the quality score from the ORES API, these rows must be removed from the main dataset to ensure accurate analysis.\n",
    "\n",
    "#### Key Steps:\n",
    "1. **Identify Rows with Missing `revision_id`:**  \n",
    "   We first created a subset of the DataFrame `rows_with_missing_revision` to capture rows where the `revision_id` is missing. These rows correspond to articles for which we could not retrieve a valid revision ID via the Wikipedia API.\n",
    "\n",
    "2. **Count Removed Rows:**  \n",
    "   Although the code allows for counting the rows with missing `revision_id`, we skip this step here, but it can be enabled as needed by uncommenting the line `count_removed_rows = len(rows_with_missing_revision)`.\n",
    "\n",
    "3. **Remove Rows with Missing `revision_id`:**  \n",
    "   Finally, the rows with missing `revision_id` are removed from the main DataFrame `merged_df_clean`, and the filtered dataset is stored in `merged_df_filtered`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#revmoing rows where revision_id is missing (also the same list as ores_error_log) and count the removed rows\n",
    "rows_with_missing_revision = merged_df_clean[merged_df_clean['revision_id'].isna()]\n",
    "# count_removed_rows = len(rows_with_missing_revision)\n",
    "\n",
    "#remove the rows with missing revision_id from the main df\n",
    "merged_df_filtered = merged_df_clean.dropna(subset=['revision_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Total Articles Per Capita and High-Quality Articles Per Capita\n",
    "\n",
    "In this section, we calculate two important metrics to analyze the coverage and quality of Wikipedia articles about politicians for each country and region:\n",
    "- **Total Articles Per Capita**: The number of Wikipedia articles about politicians per person, using the population data.\n",
    "- **High-Quality Articles Per Capita**: The number of high-quality articles (as defined by ORES with the labels \"FA\" or \"GA\") per person.\n",
    "\n",
    "#### Key Steps:\n",
    "1. **Identifying High-Quality Articles:**  \n",
    "   A new column `high_quality` is added to the DataFrame, indicating whether each article is considered high-quality based on the ORES labels \"FA\" (Featured Article) or \"GA\" (Good Article).\n",
    "\n",
    "2. **Calculating Country-Level Metrics:**  \n",
    "   The data is grouped by `country` to calculate the total number of articles and high-quality articles per country. Additionally, we retrieve the population for each country.\n",
    "\n",
    "3. **Per Capita Calculations:**  \n",
    "   - The **Total Articles Per Capita** is calculated by dividing the total number of articles by the country's population.\n",
    "   - The **High-Quality Articles Per Capita** is calculated by dividing the number of high-quality articles by the country's population.\n",
    "\n",
    "4. **Calculating Region-Level Metrics:**  \n",
    "   To ensure each country's population is only counted once, we group the data by both `region` and `country`, and then aggregate the results by region.\n",
    "   - Total articles and high-quality articles are summed for each region, and population values are aggregated for unique countries within each region.\n",
    "   - The **Total Articles Per Capita** and **High-Quality Articles Per Capita** are then calculated for each region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/g7llzl3517q8v_lqg7_6_5_40000gn/T/ipykernel_29650/939760470.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_df_filtered['high_quality'] = merged_df_filtered['article_quality'].isin(high_quality_classes)\n"
     ]
    }
   ],
   "source": [
    "#Calculating total-articles-per-capita and high-quality-articles-per-capita -->\n",
    "\n",
    "#definition of high-quality articles\n",
    "high_quality_classes = [\"FA\", \"GA\"]\n",
    "\n",
    "#calculate total articles and high-quality articles per country\n",
    "merged_df_filtered['high_quality'] = merged_df_filtered['article_quality'].isin(high_quality_classes)\n",
    "\n",
    "#group by country and calculate the required metrics\n",
    "country_grouped = merged_df_filtered.groupby('country').agg(\n",
    "    total_articles=('article_title', 'count'),\n",
    "    high_quality_articles=('high_quality', 'sum'),\n",
    "    population=('population', 'first')\n",
    ").reset_index()\n",
    "\n",
    "#total-articles-per-capita and high-quality-articles-per-capita\n",
    "country_grouped['total_articles_per_capita'] = country_grouped['total_articles'] / (country_grouped['population'])\n",
    "country_grouped['high_quality_articles_per_capita'] = country_grouped['high_quality_articles'] / (country_grouped['population'])\n",
    "\n",
    "#group by both region and country, then aggregate. \n",
    "#This helps in making sure each country's population is only counted once per regionn\n",
    "grouped = merged_df_filtered.groupby(['region', 'country']).agg(\n",
    "    total_articles=('article_title', 'count'),\n",
    "    high_quality_articles=('high_quality', 'sum'),\n",
    "    population=('population', 'first')\n",
    ").reset_index()\n",
    "\n",
    "#aggregate by region only to sum up the results from unique countries\n",
    "region_grouped = grouped.groupby('region').agg(\n",
    "    total_articles=('total_articles', 'sum'),\n",
    "    high_quality_articles=('high_quality_articles', 'sum'),\n",
    "    population=('population', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "#calculate total-articles-per-capita and high-quality-articles-per-capita for regions\n",
    "region_grouped['total_articles_per_capita'] = region_grouped['total_articles'] / (region_grouped['population'])\n",
    "region_grouped['high_quality_articles_per_capita'] = region_grouped['high_quality_articles'] / (region_grouped['population'])\n",
    "\n",
    "\n",
    "country_grouped.head(), region_grouped.head()\n",
    "region_grouped.to_csv('../data/region_grouped2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS: Top 10 Countries by Coverage\n",
    "\n",
    "In this section, we display the top 10 countries with the highest total articles per capita (normalized by population). Countries with population values of zero (resulting in infinite values for articles per capita) have been excluded. Additionally, we exclude countries where the population in millions is less than 0.05 (those countries' total articles per capita are ignored, as they would be too small to display).\n",
    "\n",
    "Below is the code used to generate the top 10 countries by coverage, ranked in descending order:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 countries by coverage-->\n",
      "+-----------------------+--------------------------------+-------------------------------------------+\n",
      "|   Rank (from the top) | country                        |   total_articles_per_capita (per million) |\n",
      "|-----------------------+--------------------------------+-------------------------------------------|\n",
      "|                     1 | Antigua and Barbuda            |                                  330      |\n",
      "|                     2 | Federated States of Micronesia |                                  140      |\n",
      "|                     3 | Marshall Islands               |                                  130      |\n",
      "|                     4 | Tonga                          |                                  100      |\n",
      "|                     5 | Barbados                       |                                   83.3333 |\n",
      "|                     6 | Montenegro                     |                                   60      |\n",
      "|                     7 | Seychelles                     |                                   60      |\n",
      "|                     8 | Bhutan                         |                                   55      |\n",
      "|                     9 | Maldives                       |                                   55      |\n",
      "|                    10 | Samoa                          |                                   40      |\n",
      "+-----------------------+--------------------------------+-------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "## RESULTS\n",
    "#producing the required tables based on the previous calculations\n",
    "\n",
    "# Top 10 countries by coverage, excluding infinite values\n",
    "\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "#filter out infinity values from the total_articles_per_capita column since their values in million is 0 \n",
    "#since the data shows upto decimal of 1, so the minimum the data would show is 0.1\n",
    "#thus any value that is lesser than 0.05 million will not show up in the data - ignoring such countries (2 in number)\n",
    "filtered_countries_coverage = country_grouped.replace([np.inf, -np.inf], np.nan).dropna(subset=['total_articles_per_capita'])\n",
    "\n",
    "#top 10 countries by coverage, excluding infinite values\n",
    "top_10_countries_coverage = filtered_countries_coverage.nlargest(10, 'total_articles_per_capita')\n",
    "top_10_countries_coverage['total_articles_per_capita (per million)'] = top_10_countries_coverage['total_articles_per_capita'].apply(lambda x: format(x, '.4f'))\n",
    "\n",
    "#add a rank column to explicitly show the ordering\n",
    "top_10_countries_coverage['Rank (from the top)'] = range(1, len(top_10_countries_coverage) + 1)\n",
    "top_10_display = top_10_countries_coverage[['Rank (from the top)', 'country', 'total_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Top 10 countries by coverage-->\")\n",
    "print(tabulate(top_10_display, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS: Bottom 10 Countries by Coverage\n",
    "\n",
    "In this section, we display the bottom 10 countries with the lowest total articles per capita (normalized by population). Countries with population values of zero (resulting in infinite values for articles per capita) have been excluded from this analysis. Below is the code used to generate the bottom 10 countries by coverage, ranked in ascending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 10 countries by coverage-->\n",
      "+--------------------------+---------------+-------------------------------------------+\n",
      "|   Rank (from the bottom) | country       |   total_articles_per_capita (per million) |\n",
      "|--------------------------+---------------+-------------------------------------------|\n",
      "|                        1 | China         |                                  0.034011 |\n",
      "|                        2 | Ghana         |                                  0.087977 |\n",
      "|                        3 | India         |                                  0.105698 |\n",
      "|                        4 | Saudi Arabia  |                                  0.135501 |\n",
      "|                        5 | Zambia        |                                  0.148515 |\n",
      "|                        6 | Norway        |                                  0.181818 |\n",
      "|                        7 | Israel        |                                  0.204082 |\n",
      "|                        8 | Egypt         |                                  0.304183 |\n",
      "|                        9 | Cote d'Ivoire |                                  0.323625 |\n",
      "|                       10 | Ethiopia      |                                  0.347826 |\n",
      "+--------------------------+---------------+-------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Bottom 10 countries by coverage\n",
    "\n",
    "#excluding infinite values\n",
    "bottom_10_countries_coverage = filtered_countries_coverage.nsmallest(10, 'total_articles_per_capita')\n",
    "\n",
    "bottom_10_countries_coverage['total_articles_per_capita (per million)'] = bottom_10_countries_coverage['total_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#add a rank column\n",
    "bottom_10_countries_coverage['Rank (from the bottom)'] = range(1, len(bottom_10_countries_coverage) + 1)\n",
    "\n",
    "bottom_10_display = bottom_10_countries_coverage[['Rank (from the bottom)', 'country', 'total_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Bottom 10 countries by coverage-->\")\n",
    "print(tabulate(bottom_10_display, headers='keys', tablefmt='psql', showindex=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS: Top 10 Countries by High-Quality Articles\n",
    "\n",
    "This section presents the top 10 countries ranked by high-quality articles per capita, where \"high-quality\" is defined as articles classified as \"FA\" (featured article) or \"GA\" (good article) based on ORES predictions. The data is normalized by population (in millions) to calculate per capita metrics. Countries are ranked in descending order of high-quality articles per capita.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 countries by high quality-->\n",
      "+-----------------------+-----------------------+--------------------------------------------------+\n",
      "|   Rank (from the top) | country               |   high_quality_articles_per_capita (per million) |\n",
      "|-----------------------+-----------------------+--------------------------------------------------|\n",
      "|                     1 | Montenegro            |                                         5        |\n",
      "|                     2 | Luxembourg            |                                         2.85714  |\n",
      "|                     3 | Albania               |                                         2.59259  |\n",
      "|                     4 | Kosovo                |                                         2.35294  |\n",
      "|                     5 | Maldives              |                                         1.66667  |\n",
      "|                     6 | Lithuania             |                                         1.37931  |\n",
      "|                     7 | Croatia               |                                         1.31579  |\n",
      "|                     8 | Guyana                |                                         1.25     |\n",
      "|                     9 | Palestinian Territory |                                         1.09091  |\n",
      "|                    10 | Slovenia              |                                         0.952381 |\n",
      "+-----------------------+-----------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Top 10 countries by high quality\n",
    "\n",
    "\n",
    "top_10_countries_high_quality = country_grouped.nlargest(10, 'high_quality_articles_per_capita')\n",
    "\n",
    "top_10_countries_high_quality['high_quality_articles_per_capita (per million)'] = top_10_countries_high_quality['high_quality_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#adding a rank column to mark the order\n",
    "top_10_countries_high_quality['Rank (from the top)'] = range(1, len(top_10_countries_high_quality) + 1)\n",
    "top_10_high_quality_display = top_10_countries_high_quality[['Rank (from the top)', 'country', 'high_quality_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Top 10 countries by high quality-->\")\n",
    "print(tabulate(top_10_high_quality_display, headers='keys', tablefmt='psql', showindex=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS: Bottom 10 Countries by High-Quality Articles\n",
    "\n",
    "This section lists the bottom 10 countries by high-quality articles per capita. \"High-quality\" articles are defined as those classified as \"FA\" (featured article) or \"GA\" (good article) based on ORES predictions. These countries have the lowest proportion of high-quality articles per capita, normalized by population (in millions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 10 countries by high quality-->\n",
      "+--------------------------+---------------------+--------------------------------------------------+\n",
      "|   Rank (from the bottom) | country             |   high_quality_articles_per_capita (per million) |\n",
      "|--------------------------+---------------------+--------------------------------------------------|\n",
      "|                        1 | Antigua and Barbuda |                                                0 |\n",
      "|                        2 | Bahamas             |                                                0 |\n",
      "|                        3 | Barbados            |                                                0 |\n",
      "|                        4 | Belize              |                                                0 |\n",
      "|                        5 | Benin               |                                                0 |\n",
      "|                        6 | Bhutan              |                                                0 |\n",
      "|                        7 | Botswana            |                                                0 |\n",
      "|                        8 | Cape Verde          |                                                0 |\n",
      "|                        9 | Chad                |                                                0 |\n",
      "|                       10 | China               |                                                0 |\n",
      "+--------------------------+---------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Bottom 10 countries by high quality\n",
    "\n",
    "bottom_10_countries_high_quality = country_grouped.nsmallest(10, 'high_quality_articles_per_capita')\n",
    "\n",
    "bottom_10_countries_high_quality['high_quality_articles_per_capita (per million)'] = bottom_10_countries_high_quality['high_quality_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#adding rank column to mark the order\n",
    "bottom_10_countries_high_quality['Rank (from the bottom)'] = range(1, len(bottom_10_countries_high_quality) + 1)\n",
    "\n",
    "# Select only the relevant columns for display\n",
    "bottom_10_high_quality_display = bottom_10_countries_high_quality[['Rank (from the bottom)', 'country', 'high_quality_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Bottom 10 countries by high quality-->\")\n",
    "print(tabulate(bottom_10_high_quality_display, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "#there are a lot of countries have articles but are not high quality articles, hence\n",
    "#showing 10 of them in ascending order of the country names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS: Geographic Regions by Total Coverage\n",
    "\n",
    "This section lists geographic regions ranked by total articles per capita, sorted in descending order. The \"total articles per capita\" metric represents the number of Wikipedia articles about political figures for each region, normalized by the population of the region (in millions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic regions by total coverage-->\n",
      "+--------+-----------------+-------------------------------------------+\n",
      "|   Rank | region          |   total_articles_per_capita (per million) |\n",
      "|--------+-----------------+-------------------------------------------|\n",
      "|      1 | NORTHERN EUROPE |                                  6.8705   |\n",
      "|      2 | OCEANIA         |                                  6.48649  |\n",
      "|      3 | CARIBBEAN       |                                  5.95628  |\n",
      "|      4 | SOUTHERN EUROPE |                                  5.26073  |\n",
      "|      5 | CENTRAL AMERICA |                                  3.66472  |\n",
      "|      6 | WESTERN EUROPE  |                                  2.74131  |\n",
      "|      7 | EASTERN EUROPE  |                                  2.66341  |\n",
      "|      8 | WESTERN ASIA    |                                  2.06161  |\n",
      "|      9 | SOUTHERN AFRICA |                                  1.80088  |\n",
      "|     10 | EASTERN AFRICA  |                                  1.38074  |\n",
      "|     11 | SOUTH AMERICA   |                                  1.33882  |\n",
      "|     12 | CENTRAL ASIA    |                                  1.31841  |\n",
      "|     13 | NORTHERN AFRICA |                                  1.18015  |\n",
      "|     14 | WESTERN AFRICA  |                                  1.17793  |\n",
      "|     15 | MIDDLE AFRICA   |                                  1.13974  |\n",
      "|     16 | SOUTHEAST ASIA  |                                  0.70023  |\n",
      "|     17 | SOUTH ASIA      |                                  0.330179 |\n",
      "|     18 | EAST ASIA       |                                  0.117745 |\n",
      "+--------+-----------------+-------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Geographic regions by total coverage\n",
    "\n",
    "#Sort the region_grouped df by total_articles_per_capita in desc order\n",
    "region_grouped_sorted_total_coverage = region_grouped.sort_values('total_articles_per_capita', ascending=False)\n",
    "\n",
    "region_grouped_sorted_total_coverage['total_articles_per_capita (per million)'] = region_grouped_sorted_total_coverage['total_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#adding a rank column to mark the order\n",
    "region_grouped_sorted_total_coverage['Rank'] = range(1, len(region_grouped_sorted_total_coverage) + 1)\n",
    "\n",
    "region_total_coverage_display = region_grouped_sorted_total_coverage[['Rank', 'region', 'total_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Geographic regions by total coverage-->\")\n",
    "print(tabulate(region_total_coverage_display, headers='keys', tablefmt='psql', showindex=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULTS: Geographic Regions by High Quality Coverage\n",
    "\n",
    "This section ranks geographic regions based on the number of high-quality Wikipedia articles per capita. High-quality articles are those classified as \"FA\" (Featured Article) or \"GA\" (Good Article) by the ORES model. The metric represents the number of high-quality articles per person (per million people) in each region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic regions by high quality coverage -->\n",
      "+--------+-----------------+--------------------------------------------------+\n",
      "|   Rank | region          |   high_quality_articles_per_capita (per million) |\n",
      "|--------+-----------------+--------------------------------------------------|\n",
      "|      1 | SOUTHERN EUROPE |                                         0.349835 |\n",
      "|      2 | NORTHERN EUROPE |                                         0.323741 |\n",
      "|      3 | CARIBBEAN       |                                         0.245902 |\n",
      "|      4 | CENTRAL AMERICA |                                         0.194932 |\n",
      "|      5 | EASTERN EUROPE  |                                         0.14275  |\n",
      "|      6 | SOUTHERN AFRICA |                                         0.11713  |\n",
      "|      7 | WESTERN EUROPE  |                                         0.11583  |\n",
      "|      8 | WESTERN ASIA    |                                         0.091401 |\n",
      "|      9 | OCEANIA         |                                         0.09009  |\n",
      "|     10 | NORTHERN AFRICA |                                         0.066432 |\n",
      "|     11 | CENTRAL ASIA    |                                         0.062189 |\n",
      "|     12 | SOUTH AMERICA   |                                         0.044706 |\n",
      "|     13 | SOUTHEAST ASIA  |                                         0.044318 |\n",
      "|     14 | MIDDLE AFRICA   |                                         0.039643 |\n",
      "|     15 | EASTERN AFRICA  |                                         0.03535  |\n",
      "|     16 | WESTERN AFRICA  |                                         0.029392 |\n",
      "|     17 | SOUTH ASIA      |                                         0.010349 |\n",
      "|     18 | EAST ASIA       |                                         0.00192  |\n",
      "+--------+-----------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Geographic regions by high quality coverage\n",
    "\n",
    "#Sort the region_grouped df by high_quality_articles_per_capita in descending order\n",
    "region_grouped_sorted_high_quality = region_grouped.sort_values('high_quality_articles_per_capita', ascending=False)\n",
    "\n",
    "region_grouped_sorted_high_quality['high_quality_articles_per_capita (per million)'] = region_grouped_sorted_high_quality['high_quality_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#adding a rank column to mark the order\n",
    "region_grouped_sorted_high_quality['Rank'] = range(1, len(region_grouped_sorted_high_quality) + 1)\n",
    "\n",
    "region_high_quality_display = region_grouped_sorted_high_quality[['Rank', 'region', 'high_quality_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Geographic regions by high quality coverage -->\")\n",
    "print(tabulate(region_high_quality_display, headers='keys', tablefmt='psql', showindex=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
