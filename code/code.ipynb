{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name                                                url  \\\n",
      "0        Majah Ha Adrif       https://en.wikipedia.org/wiki/Majah_Ha_Adrif   \n",
      "1     Haroon al-Afghani    https://en.wikipedia.org/wiki/Haroon_al-Afghani   \n",
      "2           Tayyab Agha          https://en.wikipedia.org/wiki/Tayyab_Agha   \n",
      "3  Khadija Zahra Ahmadi  https://en.wikipedia.org/wiki/Khadija_Zahra_Ah...   \n",
      "4        Aziza Ahmadyar       https://en.wikipedia.org/wiki/Aziza_Ahmadyar   \n",
      "\n",
      "       country  \n",
      "0  Afghanistan  \n",
      "1  Afghanistan  \n",
      "2  Afghanistan  \n",
      "3  Afghanistan  \n",
      "4  Afghanistan  \n",
      "         Geography  Population\n",
      "0            WORLD      8009.0\n",
      "1           AFRICA      1453.0\n",
      "2  NORTHERN AFRICA       256.0\n",
      "3          Algeria        46.8\n",
      "4            Egypt       105.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Geography     233\n",
       "Population    233\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load the politicians data\n",
    "politicians_df = pd.read_csv('../data/politicians_by_country_AUG.2024.csv')\n",
    "\n",
    "#Load the population data\n",
    "population_df = pd.read_csv('../data/population_by_country_AUG.2024.csv')\n",
    "\n",
    "#View the first few rows of each dataset\n",
    "print(politicians_df.head())\n",
    "politicians_df.count()\n",
    "\n",
    "print(population_df.head())\n",
    "population_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "44\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "#Check for duplicates based on all columns\n",
    "duplicate_politicians_all = politicians_df[politicians_df.duplicated()]\n",
    "print(len(duplicate_politicians_all))\n",
    "\n",
    "#Check for duplicates based on name + url\n",
    "duplicate_politicians_name = politicians_df[politicians_df.duplicated(subset=['name'])]\n",
    "duplicate_politicians_url = politicians_df[politicians_df.duplicated(subset=['url'])]\n",
    "print(len(duplicate_politicians_name))\n",
    "print(len(duplicate_politicians_url))\n",
    "\n",
    "#This means that the only thing setting them apart is different countries for the same name and url.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Check for duplicates based on all columns\n",
    "duplicate_population_all = population_df[population_df.duplicated()]\n",
    "print(len(duplicate_population_all))\n",
    "\n",
    "#Check for duplicates based on geography\n",
    "duplicate_population_geo = population_df[population_df.duplicated(subset=['Geography'])]\n",
    "print(len(duplicate_population_geo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44 of politician appear in mulitple countries (2 or more). This has been decided (based on the Wikipedia API)\n",
    "#either by their nationalities or the next country served, so it makes sense to have them be a part of both/all \n",
    "#the countries their names appear in\n",
    "\n",
    "# however we keep a copy of the duplicate politicians\n",
    "combined_duplicates = pd.concat([duplicate_politicians_name, duplicate_politicians_url]).drop_duplicates()\n",
    "combined_duplicates.to_csv('../data/combined_duplicates_politicians.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Geography     209\n",
       "Population    209\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create a new column to check if Geography is in all caps\n",
    "population_df['is_region'] = population_df['Geography'].apply(lambda x: x.isupper())\n",
    "\n",
    "df_region = population_df[population_df['is_region'] == True].copy()\n",
    "df_country = population_df[population_df['is_region'] == False].copy()\n",
    "\n",
    "# Drop the helper column\n",
    "df_region = df_region.drop(columns=['is_region'])\n",
    "df_country = df_country.drop(columns=['is_region'])\n",
    "\n",
    "df_country.count()\n",
    "# df_region.to_csv('population_by_region.csv', index=False)\n",
    "# df_country.to_csv('population_by_country.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country  Population Continent           Region\n",
      "0  Algeria        46.8    AFRICA  NORTHERN AFRICA\n",
      "1    Egypt       105.2    AFRICA  NORTHERN AFRICA\n",
      "2    Libya         6.9    AFRICA  NORTHERN AFRICA\n",
      "3  Morocco        37.0    AFRICA  NORTHERN AFRICA\n",
      "4    Sudan        48.1    AFRICA  NORTHERN AFRICA\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "population_df_copy = population_df.copy()\n",
    "population_df_copy['Continent'] = None\n",
    "population_df_copy['Region'] = None\n",
    "\n",
    "special_cases = {\n",
    "    'NORTHERN AMERICA': 'NORTHERN AMERICA',\n",
    "    'OCEANIA': 'OCEANIA'\n",
    "}\n",
    "\n",
    "#variables to store the current continent and region\n",
    "current_continent = None\n",
    "current_region = None\n",
    "\n",
    "#iterate over the rows of the df and assign the continent and region\n",
    "for index, row in population_df_copy.iterrows():\n",
    "    geography = row['Geography']\n",
    "    \n",
    "    if geography.isupper():\n",
    "        if geography == 'WORLD':\n",
    "            continue #skipping world\n",
    "        elif geography in ['AFRICA', 'NORTHERN AMERICA','LATIN AMERICA AND THE CARIBBEAN', 'EUROPE', 'ASIA', 'OCEANIA']:\n",
    "\n",
    "            current_continent = geography\n",
    "            current_region = special_cases.get(geography, None)\n",
    "   \n",
    "        else:\n",
    "            current_region = geography\n",
    "    else:\n",
    "        population_df_copy.at[index, 'Continent'] = current_continent\n",
    "        population_df_copy.at[index, 'Region'] = current_region\n",
    "\n",
    "population_df_copy.rename(columns={'Geography': 'Country'}, inplace=True)\n",
    "\n",
    "#filter out rows that represent continents or regions\n",
    "#we already have the regions and continents information populated, so we will drop rows where continent or region is None.\n",
    "population_df_new = population_df_copy.dropna(subset=['Continent', 'Region'])\n",
    "\n",
    "population_df_new.reset_index(drop=True, inplace=True)\n",
    "\n",
    "if 'is_region' in population_df_new.columns:\n",
    "    population_df_new = population_df_new.drop(columns=['is_region'])\n",
    "population_df_new.to_csv('../data/population_expanded.csv', index=False)\n",
    "\n",
    "print(population_df_new.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name             0\n",
      "url              0\n",
      "country          0\n",
      "revision_id      8\n",
      "quality_score    8\n",
      "dtype: int64\n",
      "Geography     0\n",
      "Population    0\n",
      "is_region     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for missing values in each column\n",
    "missing_values_politicians = politicians_df.isnull().sum()\n",
    "print(missing_values_politicians)\n",
    "\n",
    "#check for missing values in each column\n",
    "missing_values_population = population_df.isnull().sum()\n",
    "print(missing_values_population)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "#constants\n",
    "API_ENWIKIPEDIA_ENDPOINT = \"https://en.wikipedia.org/w/api.php\"\n",
    "API_HEADER_AGENT = 'User-Agent'\n",
    "API_LATENCY_ASSUMED = 0.002\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<tbaner@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2024'\n",
    "}\n",
    "\n",
    "#pageInfo Request Template\n",
    "PAGEINFO_PARAMS_TEMPLATE = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"\",  \n",
    "    \"prop\": \"info\",\n",
    "    \"inprop\": \"url|talkid\"\n",
    "}\n",
    "\n",
    "#function to request page info and retrieve revision ID\n",
    "def request_pageinfo_per_article(article_title):\n",
    "    request_template = PAGEINFO_PARAMS_TEMPLATE.copy()\n",
    "    request_template['titles'] = article_title\n",
    "\n",
    "    if API_THROTTLE_WAIT > 0.0:\n",
    "        time.sleep(API_THROTTLE_WAIT)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_ENWIKIPEDIA_ENDPOINT, headers=REQUEST_HEADERS, params=request_template)\n",
    "        json_response = response.json()\n",
    "        \n",
    "        #extract the page info\n",
    "        pages = json_response[\"query\"][\"pages\"]\n",
    "        for page_id, page_info in pages.items():\n",
    "            revision_id = page_info.get(\"lastrevid\", None)\n",
    "            if revision_id:\n",
    "                return revision_id\n",
    "            else:\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page info for {article_title}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORES API Constants\n",
    "ORES_ENDPOINT = \"https://ores.wikimedia.org/v3/scores/enwiki/\"\n",
    "ORES_MODEL = \"wp10\"  # The model used for quality predictions\n",
    "\n",
    "#function to request article quality using ORES API\n",
    "def get_ores_quality_prediction(article_title, revision_id):\n",
    "    try:\n",
    "        ores_url = f\"{ORES_ENDPOINT}?models={ORES_MODEL}&revids={revision_id}\"\n",
    "        response = requests.get(ores_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            scores = data['enwiki']['scores'][str(revision_id)]['wp10']['score']['prediction']\n",
    "            return scores\n",
    "        else:\n",
    "            print(f\"Failed to get ORES score for {article_title} (Revision ID: {revision_id})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting ORES score for {article_title}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.11%\n"
     ]
    }
   ],
   "source": [
    "#list to log articles without scores\n",
    "error_log = []\n",
    "\n",
    "#dd columns to store revision ID and quality score\n",
    "politicians_df['revision_id'] = None\n",
    "politicians_df['quality_score'] = None\n",
    "\n",
    "#loop through each politician and get revision ID and quality score\n",
    "for index, row in politicians_df.iterrows():\n",
    "    article_title = row['url'].split('/')[-1]\n",
    "    revision_id = request_pageinfo_per_article(article_title)\n",
    "    \n",
    "    if revision_id:\n",
    "        quality_score = get_ores_quality_prediction(article_title, revision_id)\n",
    "        politicians_df.at[index, 'revision_id'] = revision_id\n",
    "        politicians_df.at[index, 'quality_score'] = quality_score\n",
    "    else:\n",
    "        error_log.append(article_title)\n",
    "\n",
    "#save the error log for any missing articles\n",
    "with open('../data/ores_error_log.txt', 'w') as log_file:\n",
    "    log_file.write(\"\\n\".join(error_log))\n",
    "\n",
    "politicians_df.to_csv('../data/politicians_with_quality.csv', index=False)\n",
    "\n",
    "#calculate error rate\n",
    "error_rate = len(error_log) / len(politicians_df)\n",
    "print(f\"Error rate: {error_rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/g7llzl3517q8v_lqg7_6_5_40000gn/T/ipykernel_29650/1044880352.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_df_clean.rename(columns={\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>population</th>\n",
       "      <th>article_title</th>\n",
       "      <th>revision_id</th>\n",
       "      <th>article_quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Majah Ha Adrif</td>\n",
       "      <td>1233202991</td>\n",
       "      <td>Start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Haroon al-Afghani</td>\n",
       "      <td>1230459615</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Tayyab Agha</td>\n",
       "      <td>1225661708</td>\n",
       "      <td>Start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Khadija Zahra Ahmadi</td>\n",
       "      <td>1234741562</td>\n",
       "      <td>Stub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>SOUTH ASIA</td>\n",
       "      <td>42.4</td>\n",
       "      <td>Aziza Ahmadyar</td>\n",
       "      <td>1195651393</td>\n",
       "      <td>Start</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country      region  population         article_title revision_id  \\\n",
       "0  Afghanistan  SOUTH ASIA        42.4        Majah Ha Adrif  1233202991   \n",
       "1  Afghanistan  SOUTH ASIA        42.4     Haroon al-Afghani  1230459615   \n",
       "2  Afghanistan  SOUTH ASIA        42.4           Tayyab Agha  1225661708   \n",
       "3  Afghanistan  SOUTH ASIA        42.4  Khadija Zahra Ahmadi  1234741562   \n",
       "4  Afghanistan  SOUTH ASIA        42.4        Aziza Ahmadyar  1195651393   \n",
       "\n",
       "  article_quality  \n",
       "0           Start  \n",
       "1               B  \n",
       "2           Start  \n",
       "3            Stub  \n",
       "4           Start  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining the China entries and Guinea -Bissau ones. Leaving Korea out (as the articles do not mention \n",
    "#whether the politicians are in North or South Korea so clubbing them into Korea will add bias)\n",
    "\n",
    "#adding a mapping\n",
    "name_map = {\n",
    "    \"guineabissau\": \"Guinea-Bissau\",\n",
    "    \"GuineaBissau\": \"Guinea-Bissau\",\n",
    "    \"China (Hong Kong SAR)\": \"China\",\n",
    "    \"China (Macao SAR)\": \"China\"\n",
    "}\n",
    "\n",
    "#apply the name_map to both dfs before merging\n",
    "politicians_df['country'] = politicians_df['country'].replace(name_map)\n",
    "population_df_new['Country'] = population_df_new['Country'].replace(name_map)\n",
    "\n",
    "#extract unique countries from both DataFrames\n",
    "unique_countries_politicians = politicians_df['country'].drop_duplicates()\n",
    "unique_countries_population = population_df_new['Country'].drop_duplicates()\n",
    "\n",
    "#perform an inner merge based on unique country names\n",
    "merged_df = pd.merge(\n",
    "    unique_countries_politicians.to_frame(name='country'),\n",
    "    unique_countries_population.to_frame(name='Country'),\n",
    "    how='inner',\n",
    "    left_on='country',\n",
    "    right_on='Country'\n",
    ")\n",
    "\n",
    "#identify unmatched countries by comparing the two unique sets\n",
    "unmatched_countries_left = unique_countries_politicians[~unique_countries_politicians.isin(merged_df['country'])]\n",
    "unmatched_countries_right = unique_countries_population[~unique_countries_population.isin(merged_df['Country'])]\n",
    "\n",
    "# ombine all unmatched countries into a list\n",
    "unmatched_countries = list(unmatched_countries_left) + list(unmatched_countries_right)\n",
    "\n",
    "#saving unmatched countries\n",
    "with open('../data/wp_countries-no_match.txt', 'w') as f:\n",
    "    for country in unmatched_countries:\n",
    "        if pd.notna(country):  # Only write valid country names\n",
    "            f.write(f\"{country}\\n\")\n",
    "\n",
    "#perform full inner join between the original dfs based on the matched countries\n",
    "merged_full_df = pd.merge(\n",
    "    politicians_df,\n",
    "    population_df_new,\n",
    "    how='inner',\n",
    "    left_on='country',\n",
    "    right_on='Country'\n",
    ")\n",
    "\n",
    "#cleaning the merged data to include only required columns\n",
    "merged_df_clean = merged_full_df[['country', 'Region', 'Population', 'name', 'revision_id', 'quality_score']]\n",
    "merged_df_clean.rename(columns={\n",
    "    'Region': 'region',\n",
    "    'Population': 'population',\n",
    "    'name': 'article_title',\n",
    "    'quality_score': 'article_quality'\n",
    "}, inplace=True)\n",
    "\n",
    "merged_df_clean.to_csv('../data/wp_politicians_by_country.csv', index=False)\n",
    "merged_df_clean.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#revmoing rows where revision_id is missing (also the same list as ores_error_log) and count the removed rows\n",
    "rows_with_missing_revision = merged_df_clean[merged_df_clean['revision_id'].isna()]\n",
    "# count_removed_rows = len(rows_with_missing_revision)\n",
    "\n",
    "#remove the rows with missing revision_id from the main df\n",
    "merged_df_filtered = merged_df_clean.dropna(subset=['revision_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/g7llzl3517q8v_lqg7_6_5_40000gn/T/ipykernel_29650/939760470.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_df_filtered['high_quality'] = merged_df_filtered['article_quality'].isin(high_quality_classes)\n"
     ]
    }
   ],
   "source": [
    "#Calculating total-articles-per-capita and high-quality-articles-per-capita -->\n",
    "\n",
    "#definition of high-quality articles\n",
    "high_quality_classes = [\"FA\", \"GA\"]\n",
    "\n",
    "#calculate total articles and high-quality articles per country\n",
    "merged_df_filtered['high_quality'] = merged_df_filtered['article_quality'].isin(high_quality_classes)\n",
    "\n",
    "#group by country and calculate the required metrics\n",
    "country_grouped = merged_df_filtered.groupby('country').agg(\n",
    "    total_articles=('article_title', 'count'),\n",
    "    high_quality_articles=('high_quality', 'sum'),\n",
    "    population=('population', 'first')\n",
    ").reset_index()\n",
    "\n",
    "#total-articles-per-capita and high-quality-articles-per-capita\n",
    "country_grouped['total_articles_per_capita'] = country_grouped['total_articles'] / (country_grouped['population'])\n",
    "country_grouped['high_quality_articles_per_capita'] = country_grouped['high_quality_articles'] / (country_grouped['population'])\n",
    "\n",
    "#group by both region and country, then aggregate. \n",
    "#This helps in making sure each country's population is only counted once per regionn\n",
    "grouped = merged_df_filtered.groupby(['region', 'country']).agg(\n",
    "    total_articles=('article_title', 'count'),\n",
    "    high_quality_articles=('high_quality', 'sum'),\n",
    "    population=('population', 'first')\n",
    ").reset_index()\n",
    "\n",
    "#aggregate by region only to sum up the results from unique countries\n",
    "region_grouped = grouped.groupby('region').agg(\n",
    "    total_articles=('total_articles', 'sum'),\n",
    "    high_quality_articles=('high_quality_articles', 'sum'),\n",
    "    population=('population', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "#calculate total-articles-per-capita and high-quality-articles-per-capita for regions\n",
    "region_grouped['total_articles_per_capita'] = region_grouped['total_articles'] / (region_grouped['population'])\n",
    "region_grouped['high_quality_articles_per_capita'] = region_grouped['high_quality_articles'] / (region_grouped['population'])\n",
    "\n",
    "\n",
    "country_grouped.head(), region_grouped.head()\n",
    "region_grouped.to_csv('../data/region_grouped2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 countries by coverage-->\n",
      "+-----------------------+--------------------------------+-------------------------------------------+\n",
      "|   Rank (from the top) | country                        |   total_articles_per_capita (per million) |\n",
      "|-----------------------+--------------------------------+-------------------------------------------|\n",
      "|                     1 | Antigua and Barbuda            |                                  330      |\n",
      "|                     2 | Federated States of Micronesia |                                  140      |\n",
      "|                     3 | Marshall Islands               |                                  130      |\n",
      "|                     4 | Tonga                          |                                  100      |\n",
      "|                     5 | Barbados                       |                                   83.3333 |\n",
      "|                     6 | Montenegro                     |                                   60      |\n",
      "|                     7 | Seychelles                     |                                   60      |\n",
      "|                     8 | Bhutan                         |                                   55      |\n",
      "|                     9 | Maldives                       |                                   55      |\n",
      "|                    10 | Samoa                          |                                   40      |\n",
      "+-----------------------+--------------------------------+-------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "## RESULTS\n",
    "#producing the required tables based on the previous calculations\n",
    "\n",
    "# Top 10 countries by coverage, excluding infinite values\n",
    "\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "#filter out infinity values from the total_articles_per_capita column since their values in million is 0 \n",
    "#since the data shows upto decimal of 1, so the minimum the data would show is 0.1\n",
    "#thus any value that is lesser than 0.05 million will not show up in the data - ignoring such countries (2 in number)\n",
    "filtered_countries_coverage = country_grouped.replace([np.inf, -np.inf], np.nan).dropna(subset=['total_articles_per_capita'])\n",
    "\n",
    "#top 10 countries by coverage, excluding infinite values\n",
    "top_10_countries_coverage = filtered_countries_coverage.nlargest(10, 'total_articles_per_capita')\n",
    "top_10_countries_coverage['total_articles_per_capita (per million)'] = top_10_countries_coverage['total_articles_per_capita'].apply(lambda x: format(x, '.4f'))\n",
    "\n",
    "#add a rank column to explicitly show the ordering\n",
    "top_10_countries_coverage['Rank (from the top)'] = range(1, len(top_10_countries_coverage) + 1)\n",
    "top_10_display = top_10_countries_coverage[['Rank (from the top)', 'country', 'total_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Top 10 countries by coverage-->\")\n",
    "print(tabulate(top_10_display, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 10 countries by coverage-->\n",
      "+--------------------------+---------------+-------------------------------------------+\n",
      "|   Rank (from the bottom) | country       |   total_articles_per_capita (per million) |\n",
      "|--------------------------+---------------+-------------------------------------------|\n",
      "|                        1 | China         |                                  0.034011 |\n",
      "|                        2 | Ghana         |                                  0.087977 |\n",
      "|                        3 | India         |                                  0.105698 |\n",
      "|                        4 | Saudi Arabia  |                                  0.135501 |\n",
      "|                        5 | Zambia        |                                  0.148515 |\n",
      "|                        6 | Norway        |                                  0.181818 |\n",
      "|                        7 | Israel        |                                  0.204082 |\n",
      "|                        8 | Egypt         |                                  0.304183 |\n",
      "|                        9 | Cote d'Ivoire |                                  0.323625 |\n",
      "|                       10 | Ethiopia      |                                  0.347826 |\n",
      "+--------------------------+---------------+-------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Bottom 10 countries by coverage\n",
    "\n",
    "#excluding infinite values\n",
    "bottom_10_countries_coverage = filtered_countries_coverage.nsmallest(10, 'total_articles_per_capita')\n",
    "\n",
    "bottom_10_countries_coverage['total_articles_per_capita (per million)'] = bottom_10_countries_coverage['total_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#add a rank column\n",
    "bottom_10_countries_coverage['Rank (from the bottom)'] = range(1, len(bottom_10_countries_coverage) + 1)\n",
    "\n",
    "bottom_10_display = bottom_10_countries_coverage[['Rank (from the bottom)', 'country', 'total_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Bottom 10 countries by coverage-->\")\n",
    "print(tabulate(bottom_10_display, headers='keys', tablefmt='psql', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 countries by high quality-->\n",
      "+-----------------------+-----------------------+--------------------------------------------------+\n",
      "|   Rank (from the top) | country               |   high_quality_articles_per_capita (per million) |\n",
      "|-----------------------+-----------------------+--------------------------------------------------|\n",
      "|                     1 | Montenegro            |                                         5        |\n",
      "|                     2 | Luxembourg            |                                         2.85714  |\n",
      "|                     3 | Albania               |                                         2.59259  |\n",
      "|                     4 | Kosovo                |                                         2.35294  |\n",
      "|                     5 | Maldives              |                                         1.66667  |\n",
      "|                     6 | Lithuania             |                                         1.37931  |\n",
      "|                     7 | Croatia               |                                         1.31579  |\n",
      "|                     8 | Guyana                |                                         1.25     |\n",
      "|                     9 | Palestinian Territory |                                         1.09091  |\n",
      "|                    10 | Slovenia              |                                         0.952381 |\n",
      "+-----------------------+-----------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Top 10 countries by high quality\n",
    "\n",
    "\n",
    "top_10_countries_high_quality = country_grouped.nlargest(10, 'high_quality_articles_per_capita')\n",
    "\n",
    "top_10_countries_high_quality['high_quality_articles_per_capita (per million)'] = top_10_countries_high_quality['high_quality_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#adding a rank column to mark the order\n",
    "top_10_countries_high_quality['Rank (from the top)'] = range(1, len(top_10_countries_high_quality) + 1)\n",
    "top_10_high_quality_display = top_10_countries_high_quality[['Rank (from the top)', 'country', 'high_quality_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Top 10 countries by high quality-->\")\n",
    "print(tabulate(top_10_high_quality_display, headers='keys', tablefmt='psql', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom 10 countries by high quality-->\n",
      "+--------------------------+---------------------+--------------------------------------------------+\n",
      "|   Rank (from the bottom) | country             |   high_quality_articles_per_capita (per million) |\n",
      "|--------------------------+---------------------+--------------------------------------------------|\n",
      "|                        1 | Antigua and Barbuda |                                                0 |\n",
      "|                        2 | Bahamas             |                                                0 |\n",
      "|                        3 | Barbados            |                                                0 |\n",
      "|                        4 | Belize              |                                                0 |\n",
      "|                        5 | Benin               |                                                0 |\n",
      "|                        6 | Bhutan              |                                                0 |\n",
      "|                        7 | Botswana            |                                                0 |\n",
      "|                        8 | Cape Verde          |                                                0 |\n",
      "|                        9 | Chad                |                                                0 |\n",
      "|                       10 | China               |                                                0 |\n",
      "+--------------------------+---------------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Bottom 10 countries by high quality\n",
    "\n",
    "bottom_10_countries_high_quality = country_grouped.nsmallest(10, 'high_quality_articles_per_capita')\n",
    "\n",
    "bottom_10_countries_high_quality['high_quality_articles_per_capita (per million)'] = bottom_10_countries_high_quality['high_quality_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#adding rank column to mark the order\n",
    "bottom_10_countries_high_quality['Rank (from the bottom)'] = range(1, len(bottom_10_countries_high_quality) + 1)\n",
    "\n",
    "# Select only the relevant columns for display\n",
    "bottom_10_high_quality_display = bottom_10_countries_high_quality[['Rank (from the bottom)', 'country', 'high_quality_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Bottom 10 countries by high quality-->\")\n",
    "print(tabulate(bottom_10_high_quality_display, headers='keys', tablefmt='psql', showindex=False))\n",
    "\n",
    "#there are a lot of countries have articles but are not high quality articles, hence\n",
    "#showing 10 of them in ascending order of the country names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic regions by total coverage-->\n",
      "+--------+-----------------+-------------------------------------------+\n",
      "|   Rank | region          |   total_articles_per_capita (per million) |\n",
      "|--------+-----------------+-------------------------------------------|\n",
      "|      1 | NORTHERN EUROPE |                                  6.8705   |\n",
      "|      2 | OCEANIA         |                                  6.48649  |\n",
      "|      3 | CARIBBEAN       |                                  5.95628  |\n",
      "|      4 | SOUTHERN EUROPE |                                  5.26073  |\n",
      "|      5 | CENTRAL AMERICA |                                  3.66472  |\n",
      "|      6 | WESTERN EUROPE  |                                  2.74131  |\n",
      "|      7 | EASTERN EUROPE  |                                  2.66341  |\n",
      "|      8 | WESTERN ASIA    |                                  2.06161  |\n",
      "|      9 | SOUTHERN AFRICA |                                  1.80088  |\n",
      "|     10 | EASTERN AFRICA  |                                  1.38074  |\n",
      "|     11 | SOUTH AMERICA   |                                  1.33882  |\n",
      "|     12 | CENTRAL ASIA    |                                  1.31841  |\n",
      "|     13 | NORTHERN AFRICA |                                  1.18015  |\n",
      "|     14 | WESTERN AFRICA  |                                  1.17793  |\n",
      "|     15 | MIDDLE AFRICA   |                                  1.13974  |\n",
      "|     16 | SOUTHEAST ASIA  |                                  0.70023  |\n",
      "|     17 | SOUTH ASIA      |                                  0.330179 |\n",
      "|     18 | EAST ASIA       |                                  0.117745 |\n",
      "+--------+-----------------+-------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Geographic regions by total coverage\n",
    "\n",
    "#Sort the region_grouped df by total_articles_per_capita in desc order\n",
    "region_grouped_sorted_total_coverage = region_grouped.sort_values('total_articles_per_capita', ascending=False)\n",
    "\n",
    "region_grouped_sorted_total_coverage['total_articles_per_capita (per million)'] = region_grouped_sorted_total_coverage['total_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#adding a rank column to mark the order\n",
    "region_grouped_sorted_total_coverage['Rank'] = range(1, len(region_grouped_sorted_total_coverage) + 1)\n",
    "\n",
    "region_total_coverage_display = region_grouped_sorted_total_coverage[['Rank', 'region', 'total_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Geographic regions by total coverage-->\")\n",
    "print(tabulate(region_total_coverage_display, headers='keys', tablefmt='psql', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic regions by high quality coverage -->\n",
      "+--------+-----------------+--------------------------------------------------+\n",
      "|   Rank | region          |   high_quality_articles_per_capita (per million) |\n",
      "|--------+-----------------+--------------------------------------------------|\n",
      "|      1 | SOUTHERN EUROPE |                                         0.349835 |\n",
      "|      2 | NORTHERN EUROPE |                                         0.323741 |\n",
      "|      3 | CARIBBEAN       |                                         0.245902 |\n",
      "|      4 | CENTRAL AMERICA |                                         0.194932 |\n",
      "|      5 | EASTERN EUROPE  |                                         0.14275  |\n",
      "|      6 | SOUTHERN AFRICA |                                         0.11713  |\n",
      "|      7 | WESTERN EUROPE  |                                         0.11583  |\n",
      "|      8 | WESTERN ASIA    |                                         0.091401 |\n",
      "|      9 | OCEANIA         |                                         0.09009  |\n",
      "|     10 | NORTHERN AFRICA |                                         0.066432 |\n",
      "|     11 | CENTRAL ASIA    |                                         0.062189 |\n",
      "|     12 | SOUTH AMERICA   |                                         0.044706 |\n",
      "|     13 | SOUTHEAST ASIA  |                                         0.044318 |\n",
      "|     14 | MIDDLE AFRICA   |                                         0.039643 |\n",
      "|     15 | EASTERN AFRICA  |                                         0.03535  |\n",
      "|     16 | WESTERN AFRICA  |                                         0.029392 |\n",
      "|     17 | SOUTH ASIA      |                                         0.010349 |\n",
      "|     18 | EAST ASIA       |                                         0.00192  |\n",
      "+--------+-----------------+--------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Geographic regions by high quality coverage\n",
    "\n",
    "#Sort the region_grouped df by high_quality_articles_per_capita in descending order\n",
    "region_grouped_sorted_high_quality = region_grouped.sort_values('high_quality_articles_per_capita', ascending=False)\n",
    "\n",
    "region_grouped_sorted_high_quality['high_quality_articles_per_capita (per million)'] = region_grouped_sorted_high_quality['high_quality_articles_per_capita'].apply(lambda x: f\"{x:.6f}\")\n",
    "\n",
    "#adding a rank column to mark the order\n",
    "region_grouped_sorted_high_quality['Rank'] = range(1, len(region_grouped_sorted_high_quality) + 1)\n",
    "\n",
    "region_high_quality_display = region_grouped_sorted_high_quality[['Rank', 'region', 'high_quality_articles_per_capita (per million)']]\n",
    "\n",
    "print(\"Geographic regions by high quality coverage -->\")\n",
    "print(tabulate(region_high_quality_display, headers='keys', tablefmt='psql', showindex=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
